{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq(TF2.0).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnrNeLxMBoW+ql7zHycpYa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Seq2Seq 실습\n","\n","입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델이며, 챗봇과 기계 번역과 내용 요약, STT 등에 사용되는 대표적인 예이다.\n","\n","## 1. Seq2Seq 구조\n","__seq2seq__는 크게 인코더와 디코더라는 두 개의 모듈로 구성됩니다.\n","> __인코더__ <br>\n","입력 문장의 모든 단어들을 순차적으로 입력 받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만든다.(만들어진 벡터를 컨텍스트 벡터(context vector)라고 한다.) <br>\n","> __디코더__<br>\n","컨텍스트 벡터를 받아서 Target이 되는 단어를 한 개씩 순차적으로 출력한다.\n","\n","![인코더디코더모델](https://user-images.githubusercontent.com/46054315/161442678-2e94965c-eedf-4db8-9caf-d1246e5675f1.png)\n","\n","## 2. Seq2Seq 실습\n","* __Task :__  __영어를 프랑스어로 번역__\n","\n","### 라이브러리 임포트"],"metadata":{"id":"smUBxXOZdwtE"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"lVTuP3t7YETq","executionInfo":{"status":"ok","timestamp":1649009802806,"user_tz":-540,"elapsed":13159,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"outputs":[],"source":["import os\n","import shutil\n","import zipfile\n","\n","import pandas as pd\n","import tensorflow as tf\n","import urllib3\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","source":["### 데이터 로드"],"metadata":{"id":"rPiPIbeZj3xo"}},{"cell_type":"code","source":["http = urllib3.PoolManager()\n","url ='http://www.manythings.org/anki/fra-eng.zip'\n","filename = 'fra-eng.zip'\n","path = os.getcwd()\n","zipfilename = os.path.join(path, filename)\n","with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n","    shutil.copyfileobj(r, out_file)\n","\n","with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n","    zip_ref.extractall(path)"],"metadata":{"id":"WkGXLvhnjzsT","executionInfo":{"status":"ok","timestamp":1649009803517,"user_tz":-540,"elapsed":732,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n","del lines['lic']\n","print('전체 샘플의 개수 :',len(lines))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgNOKZSoj7G-","executionInfo":{"status":"ok","timestamp":1649009804566,"user_tz":-540,"elapsed":1058,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"66456a94-7349-4ed3-94c2-e463de49bd00"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 샘플의 개수 : 192341\n"]}]},{"cell_type":"markdown","source":["전체 샘플의 개수는 총 약 19만 2천개입니다."],"metadata":{"id":"ntSyVSKGj_Ky"}},{"cell_type":"code","source":["lines = lines.loc[:, 'src':'tar']\n","lines = lines[0:60000] # 6만개만 저장\n","lines.sample(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"J3K-EpvRj9Cp","executionInfo":{"status":"ok","timestamp":1649009804567,"user_tz":-540,"elapsed":20,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"9bb56a44-75b1-4b96-f0a7-0204d837c72a"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                        src                             tar\n","11764       They're trying.                 Elles essayent.\n","12268       We need action.     Nous avons besoin d'action.\n","4002           Tom's lucky.               Tom est chanceux.\n","5193          I'm no saint.        Je ne suis pas un saint.\n","16399      We're attorneys.            Nous sommes avocats.\n","32336   Was this your idea?           Était-ce votre idée ?\n","34779  How did the exam go?  Comment s'est passé l'examen ?\n","3193           I recovered.               Je me suis remis.\n","7328         I made a list.            J'ai fait une liste.\n","3658           Please sing.            Chante, je te prie !"],"text/html":["\n","  <div id=\"df-80b45405-9057-4eb1-b4bc-88d6eb3321da\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>src</th>\n","      <th>tar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11764</th>\n","      <td>They're trying.</td>\n","      <td>Elles essayent.</td>\n","    </tr>\n","    <tr>\n","      <th>12268</th>\n","      <td>We need action.</td>\n","      <td>Nous avons besoin d'action.</td>\n","    </tr>\n","    <tr>\n","      <th>4002</th>\n","      <td>Tom's lucky.</td>\n","      <td>Tom est chanceux.</td>\n","    </tr>\n","    <tr>\n","      <th>5193</th>\n","      <td>I'm no saint.</td>\n","      <td>Je ne suis pas un saint.</td>\n","    </tr>\n","    <tr>\n","      <th>16399</th>\n","      <td>We're attorneys.</td>\n","      <td>Nous sommes avocats.</td>\n","    </tr>\n","    <tr>\n","      <th>32336</th>\n","      <td>Was this your idea?</td>\n","      <td>Était-ce votre idée ?</td>\n","    </tr>\n","    <tr>\n","      <th>34779</th>\n","      <td>How did the exam go?</td>\n","      <td>Comment s'est passé l'examen ?</td>\n","    </tr>\n","    <tr>\n","      <th>3193</th>\n","      <td>I recovered.</td>\n","      <td>Je me suis remis.</td>\n","    </tr>\n","    <tr>\n","      <th>7328</th>\n","      <td>I made a list.</td>\n","      <td>J'ai fait une liste.</td>\n","    </tr>\n","    <tr>\n","      <th>3658</th>\n","      <td>Please sing.</td>\n","      <td>Chante, je te prie !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80b45405-9057-4eb1-b4bc-88d6eb3321da')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-80b45405-9057-4eb1-b4bc-88d6eb3321da button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-80b45405-9057-4eb1-b4bc-88d6eb3321da');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### 데이터 전처리"],"metadata":{"id":"HXMjvu74lbQi"}},{"cell_type":"markdown","source":["해당 데이터는 약 19만 2천개의 병렬 문장 샘플로 구성되어있지만 여기서는 간단히 60,000개의 샘플만 가지고 기계 번역기를 구축해보도록 하겠습니다. 우선 전체 데이터 중 60,000개의 샘플만 저장하고 현재 데이터가 어떤 구성이 되었는지 확인해보겠습니다."],"metadata":{"id":"Ra42wuylkDSH"}},{"cell_type":"code","source":["lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n","lines.sample(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"GAiG3qgqkAzP","executionInfo":{"status":"ok","timestamp":1649009804568,"user_tz":-540,"elapsed":17,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"b092dfe9-e868-4aa8-a5a6-9873888a52df"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                           src                                  tar\n","13236         Don't play dumb!        \\t Ne fais pas l'imbécile. \\n\n","2894              Go find out.                    \\t Allez voir. \\n\n","29202      I hated that movie.          \\t J'ai détesté ce film. \\n\n","49311   I want you to do this.      \\t Je veux que tu le fasses. \\n\n","30845      Paper burns easily.    \\t Le papier brûle facilement. \\n\n","12460          Who cleaned it?             \\t Qui l'a nettoyée ? \\n\n","37734     They did not listen.          \\t Ils n'ont pas écouté. \\n\n","30391      It's a bad example.      \\t C'est un mauvais exemple. \\n\n","22235       Ask your question.          \\t Posez votre question. \\n\n","54500  He didn't get her joke.  \\t Il n'a pas compris sa blague. \\n"],"text/html":["\n","  <div id=\"df-3321b887-55b0-4769-a2d4-ecb4dde2b10d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>src</th>\n","      <th>tar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13236</th>\n","      <td>Don't play dumb!</td>\n","      <td>\\t Ne fais pas l'imbécile. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>2894</th>\n","      <td>Go find out.</td>\n","      <td>\\t Allez voir. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>29202</th>\n","      <td>I hated that movie.</td>\n","      <td>\\t J'ai détesté ce film. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>49311</th>\n","      <td>I want you to do this.</td>\n","      <td>\\t Je veux que tu le fasses. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>30845</th>\n","      <td>Paper burns easily.</td>\n","      <td>\\t Le papier brûle facilement. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>12460</th>\n","      <td>Who cleaned it?</td>\n","      <td>\\t Qui l'a nettoyée ? \\n</td>\n","    </tr>\n","    <tr>\n","      <th>37734</th>\n","      <td>They did not listen.</td>\n","      <td>\\t Ils n'ont pas écouté. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>30391</th>\n","      <td>It's a bad example.</td>\n","      <td>\\t C'est un mauvais exemple. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>22235</th>\n","      <td>Ask your question.</td>\n","      <td>\\t Posez votre question. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>54500</th>\n","      <td>He didn't get her joke.</td>\n","      <td>\\t Il n'a pas compris sa blague. \\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3321b887-55b0-4769-a2d4-ecb4dde2b10d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3321b887-55b0-4769-a2d4-ecb4dde2b10d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3321b887-55b0-4769-a2d4-ecb4dde2b10d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["위의 테이블은 랜덤으로 선택된 10개의 샘플을 보여줍니다. 번역 문장에 해당되는 프랑스어 데이터는 앞서 배웠듯이 시작을 의미하는 심볼 <sos>과 종료를 의미하는 심볼 <eos>을 넣어주어야 합니다. 여기서는 <sos>와 <eos> 대신 '\\t'를 시작 심볼, '\\n'을 종료 심볼로 간주하여 추가하고 다시 데이터를 출력해보겠습니다."],"metadata":{"id":"ScpXY52fkGBo"}},{"cell_type":"code","source":["# 문자 집합 구축\n","src_vocab = set()\n","for line in lines.src: # 1줄씩 읽음\n","    for char in line: # 1개의 문자씩 읽음\n","        src_vocab.add(char)\n","\n","tar_vocab = set()\n","for line in lines.tar:\n","    for char in line:\n","        tar_vocab.add(char)"],"metadata":{"id":"__8g2pb-kKH6","executionInfo":{"status":"ok","timestamp":1649009805210,"user_tz":-540,"elapsed":656,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["문자 집합의 크기를 보겠습니다."],"metadata":{"id":"HfYk0FOEkO4w"}},{"cell_type":"code","source":["src_vocab_size = len(src_vocab)+1\n","tar_vocab_size = len(tar_vocab)+1\n","print('source 문장의 char 집합 :',src_vocab_size)\n","print('target 문장의 char 집합 :',tar_vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MB0B2n_VkMNc","executionInfo":{"status":"ok","timestamp":1649009805211,"user_tz":-540,"elapsed":24,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"8aff5e77-8934-485d-f857-f207541ea1e7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["source 문장의 char 집합 : 80\n","target 문장의 char 집합 : 105\n"]}]},{"cell_type":"markdown","source":["영어와 프랑스어는 각각 80개와 105개의 문자가 존재합니다. 이 중에서 인덱스를 임의로 부여하여 일부만 출력해봅시다. 현 상태에서 인덱스를 사용하려고 하면 에러가 납니다. 하지만 정렬하여 순서를 정해준 뒤에 인덱스를 사용하여 출력해주면 됩니다."],"metadata":{"id":"WU4RpsFLkT1I"}},{"cell_type":"code","source":["src_vocab = sorted(list(src_vocab))\n","tar_vocab = sorted(list(tar_vocab))\n","print(src_vocab[45:75])\n","print(tar_vocab[45:75])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f56a-DyekQXX","executionInfo":{"status":"ok","timestamp":1649009805212,"user_tz":-540,"elapsed":20,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"f935f0bf-2713-4b9c-a45d-6254dd2ac69a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"]}]},{"cell_type":"markdown","source":["문자 집합에 문자 단위로 저장된 것을 확인할 수 있습니다. 각 문자에 인덱스를 부여하겠습니다"],"metadata":{"id":"oYOp5k3QkclZ"}},{"cell_type":"code","source":["src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n","tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n","print(src_to_index)\n","print(tar_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgjQmeWJkYuE","executionInfo":{"status":"ok","timestamp":1649009805213,"user_tz":-540,"elapsed":14,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"e9ad7a6c-877f-414a-ee02-f9541472038a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '°': 76, 'é': 77, '’': 78, '€': 79}\n","{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"]}]},{"cell_type":"markdown","source":["인덱스가 부여된 문자 집합으로부터 갖고있는 훈련 데이터에 정수 인코딩을 수행합니다. 우선 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 수행해보고, 5개의 샘플을 출력해봅시다."],"metadata":{"id":"V7cAA50lkhOQ"}},{"cell_type":"code","source":["encoder_input = []\n","\n","# 1개의 문장\n","for line in lines.src:\n","  encoded_line = []\n","  # 각 줄에서 1개의 char\n","  for char in line:\n","    # 각 char을 정수로 변환\n","    encoded_line.append(src_to_index[char])\n","  encoder_input.append(encoded_line)\n","print('source 문장의 정수 인코딩 :',encoder_input[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XoR7lCkFkeGM","executionInfo":{"status":"ok","timestamp":1649009805618,"user_tz":-540,"elapsed":413,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"2b9a5178-0382-475b-d91d-3135ecf66bd2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n"]}]},{"cell_type":"markdown","source":["정수 인코딩이 수행된 것을 볼 수 있습니다. 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩을 수행해보겠습니다."],"metadata":{"id":"X-BLi1wfknQp"}},{"cell_type":"code","source":["decoder_input = []\n","for line in lines.tar:\n","  encoded_line = []\n","  for char in line:\n","    encoded_line.append(tar_to_index[char])\n","  decoder_input.append(encoded_line)\n","print('target 문장의 정수 인코딩 :',decoder_input[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EkCelBRbki66","executionInfo":{"status":"ok","timestamp":1649009805883,"user_tz":-540,"elapsed":269,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"0e0687cc-5b91-4388-b12b-78e103469f3d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["target 문장의 정수 인코딩 : [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"]}]},{"cell_type":"markdown","source":["정상적으로 정수 인코딩이 수행된 것을 볼 수 있습니다. 아직 정수 인코딩을 수행해야 할 데이터가 하나 더 남았습니다. 디코더의 예측값과 비교하기 위한 실제값이 필요합니다. 그런데 이 실제값에는 시작 심볼에 해당되는 <sos>가 있을 필요가 없습니다. 이해가 되지 않는다면 이전 페이지의 그림으로 돌아가 Dense와 Softmax 위에 있는 단어들을 다시 보시기 바랍니다. 그래서 이번에는 정수 인코딩 과정에서 <sos>를 제거합니다. 즉, 모든 프랑스어 문장의 맨 앞에 붙어있는 '\\t'를 제거하도록 합니다."],"metadata":{"id":"odgJgW9CkuGX"}},{"cell_type":"code","source":["decoder_target = []\n","for line in lines.tar:\n","  timestep = 0\n","  encoded_line = []\n","  for char in line:\n","    if timestep > 0:\n","      encoded_line.append(tar_to_index[char])\n","    timestep = timestep + 1\n","  decoder_target.append(encoded_line)\n","print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGxMNftjkouq","executionInfo":{"status":"ok","timestamp":1649009806669,"user_tz":-540,"elapsed":787,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"d736fa18-e078-4c01-8130-cc092369d3ef"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["target 문장 레이블의 정수 인코딩 : [[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"]}]},{"cell_type":"markdown","source":["앞서 먼저 만들었던 디코더의 입력값에 해당되는 decoder_input 데이터와 비교하면 decoder_input에서는 모든 문장의 앞에 붙어있던 숫자 1이 decoder_target에서는 제거된 것을 볼 수 있습니다. '\\t'가 인덱스가 1이므로 정상적으로 제거된 것입니다. 모든 데이터에 대해서 정수 인덱스로 변경하였으니 패딩 작업을 수행합니다. 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 확인합니다."],"metadata":{"id":"ZOvRRn-Gkyrf"}},{"cell_type":"code","source":["max_src_len = max([len(line) for line in lines.src])\n","max_tar_len = max([len(line) for line in lines.tar])\n","print('source 문장의 최대 길이 :',max_src_len)\n","print('target 문장의 최대 길이 :',max_tar_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgzwujoykvvb","executionInfo":{"status":"ok","timestamp":1649009806670,"user_tz":-540,"elapsed":4,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"bc54e1bb-62e6-4dca-d68b-c05da4be91b6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["source 문장의 최대 길이 : 23\n","target 문장의 최대 길이 : 76\n"]}]},{"cell_type":"markdown","source":["각각 23와 76의 길이를 가집니다. 이번 병렬 데이터는 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩을 할 때도 이 두 개의 데이터의 __`길이를 전부 동일하게 맞춰줄 필요는 없습니다. 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩하면 됩니다.`__ 여기서는 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 23이 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩합니다."],"metadata":{"id":"R6xnVo7rk5m6"}},{"cell_type":"code","source":["encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n","decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n","decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"],"metadata":{"id":"woeRHP6nlCNs","executionInfo":{"status":"ok","timestamp":1649009807474,"user_tz":-540,"elapsed":807,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["모든 값에 대해서 원-핫 인코딩을 수행합니다. 문자 단위 번역기므로 워드 임베딩은 별도로 사용되지 않으며, 예측값과의 오차 측정에 사용되는 실제값뿐만 아니라 입력값도 원-핫 벡터를 사용하겠습니다."],"metadata":{"id":"-fywf76AlOa_"}},{"cell_type":"code","source":["encoder_input = to_categorical(encoder_input)\n","decoder_input = to_categorical(decoder_input)\n","decoder_target = to_categorical(decoder_target)"],"metadata":{"id":"lB88jZiLlKky","executionInfo":{"status":"ok","timestamp":1649009810808,"user_tz":-540,"elapsed":3341,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["데이터에 대한 전처리가 모두 끝났습니다. 본격적으로 seq2seq 모델을 설계해보겠습니다."],"metadata":{"id":"50010s4llXW_"}},{"cell_type":"markdown","source":["### 모델링\n","데이터 전치리 시 decoder input이 왜 필요한지 의문이 있을 것이다. 훈련 과정에서 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 __`교사 강요`__라고 합니다.\n"],"metadata":{"id":"93FEz4wBlhH_"}},{"cell_type":"code","source":["# 라이브러리 임포트\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.backend import clear_session\n","import numpy as np"],"metadata":{"id":"K5u-crQOlSyb","executionInfo":{"status":"ok","timestamp":1649009810809,"user_tz":-540,"elapsed":9,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["__인코더 모델링__"],"metadata":{"id":"grD64sJAnA2i"}},{"cell_type":"code","source":["clear_session()\n","\n","encoder_inputs = Input(shape=(None, src_vocab_size))\n","encoder_lstm = LSTM(units=256, return_state=True)\n","\n","# encoder_outputs은 여기서는 불필요\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n","\n","# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n","encoder_states = [state_h, state_c]"],"metadata":{"id":"4mKO1o3pmW-v","executionInfo":{"status":"ok","timestamp":1649009814408,"user_tz":-540,"elapsed":3606,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["인코더를 주목해보면 functional API를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다. 우선 LSTM의 은닉 상태 크기는 256으로 선택하였습니다. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 __`return_state=True`__로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴합니다.\n","<br>\n","LSTM에서 __`state_h, state_c`__를 리턴받는데, 이는 각각 LSTM을 설명할 때 언급하였던 배운 은닉 상태와 셀 상태에 해당됩니다. 앞서 이론을 설명할 때는 셀 상태는 설명에서 생략하고 은닉 상태만 언급하였으나 사실 LSTM은 은닉 상태와 셀 상태라는 두 가지 상태를 가진다는 사실을 기억해야 합니다. 갑자기 어려워진 게 아닙니다. 단지 은닉 상태만 전달하는 게 아니라 은닉 상태와 셀 상태 두 가지를 전달한다고 생각하면 됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달합니다. 이것이 앞서 배운 __`컨텍스트 벡터`__입니다.\n"],"metadata":{"id":"SS4LP5LQmnGw"}},{"cell_type":"markdown","source":["__디코더 모델링__"],"metadata":{"id":"b7nXK5sqnGg2"}},{"cell_type":"code","source":["decoder_inputs = Input(shape=(None, tar_vocab_size))\n","decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n","\n","# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n","decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","\n","# decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n","# decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","decoder_outputs = Dense(tar_vocab_size, activation='softmax')(decoder_outputs)"],"metadata":{"id":"nH-wXHrAmi7F","executionInfo":{"status":"ok","timestamp":1649009814803,"user_tz":-540,"elapsed":422,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용합니다. 위에서 initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 또한 동일하게 디코더의 은닉 상태 크기도 256으로 주었습니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. 그 후 출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구합니다."],"metadata":{"id":"OVKn1VmHndkH"}},{"cell_type":"markdown","source":["__모델 빌드__"],"metadata":{"id":"G4-Ah_wOnN5d"}},{"cell_type":"code","source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn-hIOCjnTNI","executionInfo":{"status":"ok","timestamp":1649009814803,"user_tz":-540,"elapsed":8,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"195e54f7-efd1-48b9-ed72-8437889f0c11"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None, 80)]   0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None, 105)]  0           []                               \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        345088      ['input_1[0][0]']                \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 256),  370688      ['input_2[0][0]',                \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 105)    26985       ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 742,761\n","Trainable params: 742,761\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["__모델 컴파일__"],"metadata":{"id":"DkQuSjo2nYKW"}},{"cell_type":"code","source":["model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"],"metadata":{"id":"2aYxsLXXnMsb","executionInfo":{"status":"ok","timestamp":1649009814804,"user_tz":-540,"elapsed":4,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["__모델 학습__"],"metadata":{"id":"N-JgJw80oE7X"}},{"cell_type":"code","source":["history = model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=5, validation_split=0.2,verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kE08YxYCnaGu","executionInfo":{"status":"ok","timestamp":1649010023533,"user_tz":-540,"elapsed":208139,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"bbce22aa-1818-439f-ad2e-14b743efed2f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","750/750 [==============================] - 40s 47ms/step - loss: 0.7400 - val_loss: 0.6622\n","Epoch 2/5\n","750/750 [==============================] - 33s 44ms/step - loss: 0.4532 - val_loss: 0.5361\n","Epoch 3/5\n","750/750 [==============================] - 33s 44ms/step - loss: 0.3799 - val_loss: 0.4691\n","Epoch 4/5\n","750/750 [==============================] - 33s 44ms/step - loss: 0.3373 - val_loss: 0.4308\n","Epoch 5/5\n","750/750 [==============================] - 33s 44ms/step - loss: 0.3091 - val_loss: 0.4062\n"]}]},{"cell_type":"markdown","source":["입력으로는 인코더 입력과 디코더 입력이 들어가고, 디코더의 실제값인 decoder_target도 필요합니다. 배치 크기는 64로 하였으며 총 40 에포크를 학습합니다. 위에서 설정한 은닉 상태의 크기와 에포크 수는 실제로는 훈련 데이터에 과적합 상태를 불러옵니다. 중간부터 검증 데이터에 대한 오차인 val_loss의 값이 올라가는데, 사실 이번 실습에서는 주어진 데이터의 양과 태스크의 특성으로 인해 훈련 과정에서 훈련 데이터의 정확도와 과적합 방지라는 두 마리 토끼를 동시에 잡기에는 쉽지 않습니다. 여기서는 우선 seq2seq의 메커니즘과 짧은 문장과 긴 문장에 대한 성능 차이에 대한 확인을 중점으로 두고 훈련 데이터에 과적합 된 상태로 동작 단계로 넘어갑니다."],"metadata":{"id":"I7-XjTuLoT3P"}},{"cell_type":"markdown","source":["### 추론\n","\n","앞서 seq2seq는 훈련할 때와 동작할 때의 방식이 다르다고 언급한 바 있습니다. 이번에는 입력한 문장에 대해서 기계 번역을 하도록 모델을 조정하고 동작시켜보도록 하겠습니다.\n","\n","전체적인 번역 동작 단계를 정리하면 아래와 같습니다.\n","1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n","2. 상태와 <SOS>에 해당하는 '\\t'를 디코더로 보냅니다.\n","3. 디코더가 <EOS>에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다."],"metadata":{"id":"LJCQ-9o6o5NO"}},{"cell_type":"markdown","source":["---\n","우선 인코더를 정의합니다. encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용하는 것입니다."],"metadata":{"id":"PvPZLiAxqAEm"}},{"cell_type":"code","source":["encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"],"metadata":{"id":"SzaF14sKoHj6","executionInfo":{"status":"ok","timestamp":1649010026605,"user_tz":-540,"elapsed":263,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":[" 디코더를 설계해보겠습니다."],"metadata":{"id":"yy5pKRxdqKDG"}},{"cell_type":"code","source":["# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n","# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n","decoder_states = [state_h, state_c]\n","# decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","decoder_outputs = Dense(tar_vocab_size, activation='softmax')(decoder_outputs)\n","decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"],"metadata":{"id":"vqJpTFeBp7QV","executionInfo":{"status":"ok","timestamp":1649010125530,"user_tz":-540,"elapsed":646,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만든다."],"metadata":{"id":"n5Xcv1S1qdW0"}},{"cell_type":"code","source":["index_to_src = dict((i, char) for char, i in src_to_index.items())\n","index_to_tar = dict((i, char) for char, i in tar_to_index.items())"],"metadata":{"id":"0tPpC54_qx_e","executionInfo":{"status":"ok","timestamp":1649010227389,"user_tz":-540,"elapsed":7,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 상태를 얻음\n","  states_value = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 원-핫 벡터 생성\n","  target_seq = np.zeros((1, 1, tar_vocab_size))\n","  target_seq[0, 0, tar_to_index['\\t']] = 1.\n","\n","  stop_condition = False\n","  decoded_sentence = \"\"\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","    # 예측 결과를 문자로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    # 현재 시점의 예측 문자를 예측 문장에 추가\n","    decoded_sentence += sampled_char\n","\n","    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","    if (sampled_char == '\\n' or\n","        len(decoded_sentence) > max_tar_len):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1, 1, tar_vocab_size))\n","    target_seq[0, 0, sampled_token_index] = 1.\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h, c]\n","\n","  return decoded_sentence"],"metadata":{"id":"UCWQJccip7UP","executionInfo":{"status":"ok","timestamp":1649010228380,"user_tz":-540,"elapsed":2,"user":{"displayName":"김종원","userId":"17740146516996869558"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["__테스트__"],"metadata":{"id":"fZE1mZtdqptJ"}},{"cell_type":"code","source":["for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n","  input_seq = encoder_input[seq_index:seq_index+1]\n","  decoded_sentence = decode_sequence(input_seq)\n","  print(35 * \"-\")\n","  print('입력 문장:', lines.src[seq_index])\n","  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n","  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEoVfqiDp7YO","executionInfo":{"status":"ok","timestamp":1649010249581,"user_tz":-540,"elapsed":20212,"user":{"displayName":"김종원","userId":"17740146516996869558"}},"outputId":"370da145-ae18-4c51-f758-a28f9b7c2e69"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------\n","입력 문장: Hi.\n","정답 문장: Salut ! \n","번역 문장: téùbFSSFFccCC$ \"o (îÊR%ÔîÊR%ÔP\t‘CÊÊR%fÊE​ûLjO\t4jO.'Or\t\t\t'.'OÀdSS»C\n","-----------------------------------\n","입력 문장: I see.\n","정답 문장: Aha. \n","번역 문장: :àUtr\tT\tÇ4\t\t\t\t\t\t\t%%%%ÊÊR%fÊyA%ÊÊR%fcccTcFpp$pp$pp$àpppààpppeùpcàûpcûû'û''''\n","-----------------------------------\n","입력 문장: Hug me.\n","정답 문장: Serrez-moi dans vos bras ! \n","번역 문장: téùbFSSFFc5CC$.fO\t\tC%.'%%9dS»\n","-----------------------------------\n","입력 문장: Help me.\n","정답 문장: Aidez-moi. \n","번역 문장: ​%.»œ%%4​%%%%%\tE'E'1222222))\n","-----------------------------------\n","입력 문장: I am sure.\n","정답 문장: Je suis sûr. \n","번역 문장: :àUtr\tT\tÇ4\t\t\t\t\t\t\t\t\t\t6%ÊR%fccxT\tÊ.'Or2\t\t\t\t\t\t\t\t\t.'MM.'M2nS»»\n"]}]},{"cell_type":"markdown","source":["## 참고 문헌\n","https://wikidocs.net/24996"],"metadata":{"id":"vitJh68xq6Xt"}}]}